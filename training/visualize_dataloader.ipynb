{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataloader\n",
    "\n",
    "This script creates the dataloader and let the user investigate the items in the dataloader by clicking on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to the directory containing images.\n",
    "            mask_dir (str): Path to the directory containing instance masks.\n",
    "            transform (callable, optional): Optional transform to be applied to images.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])[:100] # only consider first 100\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith(\".png\")])[:100]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of files in the image dataset (each image correspond to one mask)\n",
    "        \"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the images and masks\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        mask_name = image_name  # Image and mask have the same filename\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        if not mask_name:\n",
    "            return None  # No mask found, handle accordingly\n",
    "        \n",
    "        # Set final image sizes (650x700), which includes all sizes\n",
    "        image_size_h = 650\n",
    "        image_size_w = 700\n",
    "\n",
    "        # Load image\n",
    "        image_original = cv2.imread(image_path, cv2.IMREAD_UNCHANGED) / 255 # reading as is\n",
    "\n",
    "        # Pad image to match desired size\n",
    "        original_h, original_w, _ = image_original.shape\n",
    "        pad_h = max(0, (image_size_h - original_h) // 2)\n",
    "        pad_w = max(0, (image_size_w - original_w) // 2)\n",
    "\n",
    "        image_padded = np.pad(image_original, ((pad_h, image_size_h - original_h - pad_h), (pad_w, image_size_w - original_w - pad_w), (0, 0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        # Load mask (grayscale) and expand values\n",
    "        mask_original = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED) / 255\n",
    "        \n",
    "        # Pad mask to match desired size instead of interpolation\n",
    "        original_h, original_w = mask_original.shape\n",
    "\n",
    "        pad_h = max(0, (image_size_h - original_h) // 2)\n",
    "        pad_w = max(0, (image_size_w - original_w) // 2)\n",
    "\n",
    "        mask_padded = np.pad(mask_original, ((pad_h, image_size_h - original_h - pad_h), (pad_w, image_size_w - original_w - pad_w)), mode='constant', constant_values=0)\n",
    "        \n",
    "        # Ensure mask values remain categorical (0 to 255 after expansion)\n",
    "        mask_tensor = torch.tensor(mask_padded, dtype=torch.float32)\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        image_tensor = torch.tensor(image_padded, dtype=torch.float32).permute(2, 0, 1) # in tensors, channels must be first dimension\n",
    "\n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_val_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/val/images\"\n",
    "image_train_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/train/images\"\n",
    "\n",
    "mask_val_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/val/masks\"\n",
    "mask_train_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/train/masks\"\n",
    "\n",
    "instances_val_dir = \"/home/maver02/Development/Datasets/COCO/annotations/instances_val2017.json\"\n",
    "instances_train_dir = \"/home/maver02/Development/Datasets/COCO/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = COCOSegmentationDataset(image_val_dir, mask_val_dir)\n",
    "train_data = test_data # use test data for now as it is smaller\n",
    "\n",
    "batch_size = 2  # Reduce to avoid OOM\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data in Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file with instances\n",
    "with open(instances_val_dir, 'r') as file:\n",
    "    val_instances_json = json.load(file)\n",
    "\n",
    "# Create a dict mapping categories id (pixel values) into their names\n",
    "categories_dict = {}\n",
    "categories_dict[0] = 'unknown'\n",
    "for category in val_instances_json['categories']:\n",
    "    categories_dict[category['id']] = category['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel value: 0.0. Category: unknown\n",
      "Pixel value: 0.003921568859368563. Category: person\n",
      "Pixel value: 0.003921568859368563. Category: person\n",
      "Pixel value: 0.1725490242242813. Category: bottle\n",
      "Pixel value: 0.1725490242242813. Category: bottle\n",
      "Pixel value: 0.003921568859368563. Category: person\n"
     ]
    }
   ],
   "source": [
    "# Select next item in dataloader\n",
    "test_images, test_masks = next(iter(test_dataloader))\n",
    "\n",
    "# Select first object from dataloader item\n",
    "image_np = test_images[0].numpy().transpose(1, 2, 0)\n",
    "mask_np = test_masks[0].numpy()\n",
    "\n",
    "# Convert grayscale image to 3-channel\n",
    "mask_np_3ch = cv2.cvtColor(mask_np, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# Concatenate images horizontally\n",
    "img_combined = np.hstack((image_np, mask_np_3ch))\n",
    "\n",
    "# Mouse callback function\n",
    "def get_pixel_value(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # Left mouse button click\n",
    "        pixel_value = img_combined[y, x]  # OpenCV uses (y, x) indexing\n",
    "        print(f\"Pixel value: {pixel_value[0]}. Category: {categories_dict[int(pixel_value[0]*255)]}\") # print the category assigned to that pixel value\n",
    "\n",
    "# Ensure the window is created before setting the callback\n",
    "cv2.namedWindow(\"Image\")\n",
    "\n",
    "# Set the mouse callback function\n",
    "cv2.setMouseCallback(\"Image\", get_pixel_value)\n",
    "\n",
    "# Show result\n",
    "cv2.imshow(\"Image\", img_combined)\n",
    "cv2.waitKey(15000) # set timer, close by pressing any button\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-coco-object-detection-UEYJRS-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
