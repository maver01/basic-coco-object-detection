{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training - Semantic Segmentation\n",
    "\n",
    "This script performs the semantic segmentation model training of the Coco dataset.\n",
    "\n",
    "It loads the pre-processed data from the directory, build the model network, train it.\n",
    "\n",
    "- It uses masks that were built assigning each pixel in the image to its belonging classes (one mask for each image).\n",
    "- It uses Sparse Cross Entropy (because of the point above, and the fact that each pixel only belongs to one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import PyQt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to the directory containing images.\n",
    "            mask_dir (str): Path to the directory containing instance masks.\n",
    "            transform (callable, optional): Optional transform to be applied to images.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".jpg\")])\n",
    "        self.mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith(\".jpg\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of files in the image dataset (each image correspond to one mask)\n",
    "        \"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the images and masks\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        mask_name = image_name  # Image and mask have the same filename\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        if not mask_name:\n",
    "            return None  # No mask found, handle accordingly\n",
    "\n",
    "        # Load image\n",
    "        image_original = cv2.imread(image_path)\n",
    "        image_normalized = image_original.astype(np.float32) / 255.0  # Normalize\n",
    "        \n",
    "        # Pad image to match desired size\n",
    "        original_h, original_w, _ = image_normalized.shape\n",
    "        pad_h = max(0, (700 - original_h) // 2)\n",
    "        pad_w = max(0, (700 - original_w) // 2)\n",
    "        image_padded = np.pad(image_normalized, ((pad_h, 700 - original_h - pad_h), (pad_w, 700 - original_w - pad_w), (0, 0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        # Load mask (grayscale) and expand values\n",
    "        mask_original = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask_normalized = (mask_original / 1) * 1  # Expand values between min and max (keep original for now)\n",
    "        \n",
    "        # Pad mask to match desired size instead of interpolation\n",
    "        original_h, original_w = mask_normalized.shape\n",
    "        pad_h = max(0, (700 - original_h) // 2)\n",
    "        pad_w = max(0, (700 - original_w) // 2)\n",
    "        mask_padded = np.pad(mask_normalized, ((pad_h, 700 - original_h - pad_h), (pad_w, 700 - original_w - pad_w)), mode='constant', constant_values=0)\n",
    "        \n",
    "        # Ensure mask values remain categorical (0 to 255 after expansion)\n",
    "        mask_tensor = torch.tensor(mask_padded, dtype=torch.long)  # Keep it as an integer tensor\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        image_tensor = torch.tensor(image_padded, dtype=torch.float32).permute(2, 0, 1)  # Shape (C, H, W)\n",
    "\n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_val_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/val/images\"\n",
    "mask_val_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/val/masks\"\n",
    "image_train_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/train/images\"\n",
    "masks_train_dir = \"/home/maver02/Development/Datasets/COCO/preprocess_coco_2_v1/train/masks\"\n",
    "instances_val_dir = \"/home/maver02/Development/Datasets/COCO/annotations/instances_val2017.json\"\n",
    "instances_train_dir = \"/home/maver02/Development/Datasets/COCO/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = COCOSegmentationDataset(image_val_dir, mask_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file with instances\n",
    "with open(instances_val_dir, 'r') as file:\n",
    "    val_instances_json = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_instances_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict mapping categories id (pixel values) into their names\n",
    "categories_dict = {}\n",
    "categories_dict[0] = 'unknown'\n",
    "for category in val_instances_json['categories']:\n",
    "    categories_dict[category['id']] = category['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data in Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 700, 700])\n",
      "Labels batch shape: torch.Size([64, 700, 700])\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "test_features, test_masks = next(iter(test_dataloader))\n",
    "print(f\"Feature batch shape: {test_features.size()}\")\n",
    "print(f\"Labels batch shape: {test_masks.size()}\")\n",
    "\n",
    "image = test_features[0].permute(1, 2, 0).numpy()  # Convert (C, H, W) to (H, W, C)\n",
    "mask = test_masks[0].numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display image\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(image)\n",
    "ax1.set_title(\"Image\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "# Display mask\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(mask, cmap=\"gray\")\n",
    "ax2.set_title(\"Mask\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "# Function to show pixel values on hover\n",
    "def format_coord_image(x, y):\n",
    "    x, y = int(x), int(y)\n",
    "    if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n",
    "        pixel_value = image[y, x]\n",
    "        return f\"x={x}, y={y}, RGB={pixel_value}\"\n",
    "    return \"\"\n",
    "\n",
    "def format_coord_mask(x, y):\n",
    "    x, y = int(x), int(y)\n",
    "    if 0 <= x < mask.shape[1] and 0 <= y < mask.shape[0]:\n",
    "        pixel_value = mask[y, x]\n",
    "        category = categories_dict.get(int(pixel_value /1 * 1), \"Unknown\")  # Convert to category, re normalize value\n",
    "        return f\"x={x}, y={y}, Category={category}\"\n",
    "    return \"\"\n",
    "\n",
    "ax1.format_coord = format_coord_image\n",
    "ax2.format_coord = format_coord_mask\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-coco-object-detection-UEYJRS-l-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
